# Bolt

This application provides an example XGBRegressor inference generated by [bolt](https://github.com/aedalzotto/bolt)

The `dataset_t` structure provides the dataset to test.
The first fields of the struct are the model input features.
The last field if the target variable.

The inference runs for every entry of the `data` array using the input features in the `dataset_t` struct.
Every model function call returns a predicted latency.
The error is obtained as the difference from the target variable.

The error is squared and accumulated to result in the squared log error.
The squared lgog error is divided by the number of entries in the `data` vector to result in the Mean Squared Log Error (MSLE).
The square root of the MSLE results in the RMS Log Error, i.e., how far in average a predicted value is from the correct one.

This example uses the [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques) kaggle competition [example using XGBoost Regression](https://www.kaggle.com/code/carlmcbrideellis/an-introduction-to-xgboost-regression).

Note that the dataset is pre-processed so null values are represented as -1.
This could be done because all valid values were positives.
So, we check if the value is -1 and pass a NULL pointer, which is correctly treated by the estimator.
